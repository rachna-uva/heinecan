{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url= 'https://scholar.google.com/scholar'\n",
    "\n",
    "#set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' \n",
    "    #english\n",
    "}\n",
    "\n",
    "params = {\n",
    "    #english\n",
    "    'hl': 'en',\n",
    "\n",
    "    'start': 0,\n",
    "    'q': 'heat stress guidlines interventions mitigation strategies' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac07878e14b40d6bdfe9fd571cd2b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##GET TITLE/LINK/AUTHORS OF N*10 ARTICLES\n",
    "\n",
    "#loop through pages for n pages\n",
    "n = 1\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#store results\n",
    "results = pd.DataFrame(columns=['title', 'link', 'author'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i*10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    #get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "    \n",
    "    #add to results\n",
    "    results = results._append(pd.DataFrame({'title': titles, 'link': links, 'author': authors}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('article_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC CODE TO SCRAPE ABSTRACT \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_abstract(url):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        abstract = soup.find('blockquote', class_='abstract').text.strip()\n",
    "    \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.sciencedirect.com/science/article/pii/S0921800919302976'\n",
    "\n",
    "response = requests.get(link, headers=headers)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"request\"]\"}\n",
      "  (Session info: chrome=134.0.6998.166); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6C1824C25+3179557]\n",
      "\t(No symbol) [0x00007FF6C14888A0]\n",
      "\t(No symbol) [0x00007FF6C13191CA]\n",
      "\t(No symbol) [0x00007FF6C136FA67]\n",
      "\t(No symbol) [0x00007FF6C136FC9C]\n",
      "\t(No symbol) [0x00007FF6C13C3627]\n",
      "\t(No symbol) [0x00007FF6C1397C6F]\n",
      "\t(No symbol) [0x00007FF6C13C02F3]\n",
      "\t(No symbol) [0x00007FF6C1397A03]\n",
      "\t(No symbol) [0x00007FF6C13606D0]\n",
      "\t(No symbol) [0x00007FF6C1361983]\n",
      "\tGetHandleVerifier [0x00007FF6C18867CD+3579853]\n",
      "\tGetHandleVerifier [0x00007FF6C189D1D2+3672530]\n",
      "\tGetHandleVerifier [0x00007FF6C1892153+3627347]\n",
      "\tGetHandleVerifier [0x00007FF6C15F092A+868650]\n",
      "\t(No symbol) [0x00007FF6C1492FFF]\n",
      "\t(No symbol) [0x00007FF6C148F4A4]\n",
      "\t(No symbol) [0x00007FF6C148F646]\n",
      "\t(No symbol) [0x00007FF6C147EAA9]\n",
      "\tBaseThreadInitThunk [0x00007FFD6BE2E8D7+23]\n",
      "\tRtlUserThreadStart [0x00007FFD6CA1BF6C+44]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## CODE TO DOWNLOAD 1 LINK\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "main_page_url = 'https://sci-hub.ru/'\n",
    "url_to_search = 'https://www.sciencedirect.com/science/article/pii/S0921800919302976'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open page\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(2)  # Wait for the page to load (you can decrease this time if need be)\n",
    "\n",
    "    #doi_input = driver.find_element()\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)  # Wait for a bit again\n",
    "\n",
    "    # Find download link\n",
    "    download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "    onclick_attribute = download_button.get_attribute('onclick')\n",
    "\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    \n",
    "    download_link = 'https://' + urlparse(main_page_url).hostname + pdf_url\n",
    "  \n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "\n",
    "    with open('pdf.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle the exception, print an error message, etc.\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import random\n",
    "\n",
    "def pdf_download(url_to_search, file_name):\n",
    "    driver = webdriver.Chrome()  # initialize web driver\n",
    "    main_page_url = 'https://sci-hub.ru/'  # open sci-hub\n",
    "    driver.get(main_page_url)\n",
    "\n",
    "    try:\n",
    "        # Wait until the search bar with id=\"request\" is present\n",
    "        search_bar = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, 'request'))\n",
    "        )\n",
    "        search_bar.send_keys(url_to_search)\n",
    "        search_bar.send_keys(Keys.RETURN)  # Submit the search query\n",
    "    except Exception as e:\n",
    "        print(f\"Could not find search bar for {url_to_search}. Error: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    time.sleep(random.uniform(4, 6))\n",
    "\n",
    "    try:\n",
    "        # Find download button\n",
    "        download_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[@onclick]\"))\n",
    "        )\n",
    "        onclick_attribute = download_button.get_attribute('onclick')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not find download button for {url_to_search}. Error: {e}\")\n",
    "        driver.quit()\n",
    "        return url_to_search  # Mark this URL as problematic\n",
    "\n",
    "    try:\n",
    "        # Extract URL from onclick attribute\n",
    "        pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "        download_link = 'https://sci-hub.ru' + pdf_url\n",
    "\n",
    "        # Ensure 'pdfs' folder exists\n",
    "        os.makedirs('pdfs', exist_ok=True)\n",
    "\n",
    "        # Download and save the PDF\n",
    "        response = requests.get(download_link)\n",
    "        with open(f'pdfs/{file_name}.pdf', 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or save PDF for {url_to_search}. Error: {e}\")\n",
    "        driver.quit()\n",
    "        return url_to_search\n",
    "\n",
    "    driver.quit()\n",
    "    return None  # Success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to call on all links !!! DO NOT RERUN, IT WILL SET LIST BACK TO NONE\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('article_info.csv')\n",
    "title = data['title']\n",
    "links = data['link']\n",
    "author = ['author'] \n",
    "\n",
    "\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "list_error_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already downloaded\n",
      "Could not find download button for https://www.jstage.jst.go.jp/article/indhealth/51/1/51_2012-0089/_article/-char/ja/. Error: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6C1824C25+3179557]\n",
      "\t(No symbol) [0x00007FF6C14888A0]\n",
      "\t(No symbol) [0x00007FF6C13191CA]\n",
      "\t(No symbol) [0x00007FF6C136FA67]\n",
      "\t(No symbol) [0x00007FF6C136FC9C]\n",
      "\t(No symbol) [0x00007FF6C13C3627]\n",
      "\t(No symbol) [0x00007FF6C1397C6F]\n",
      "\t(No symbol) [0x00007FF6C13C02F3]\n",
      "\t(No symbol) [0x00007FF6C1397A03]\n",
      "\t(No symbol) [0x00007FF6C13606D0]\n",
      "\t(No symbol) [0x00007FF6C1361983]\n",
      "\tGetHandleVerifier [0x00007FF6C18867CD+3579853]\n",
      "\tGetHandleVerifier [0x00007FF6C189D1D2+3672530]\n",
      "\tGetHandleVerifier [0x00007FF6C1892153+3627347]\n",
      "\tGetHandleVerifier [0x00007FF6C15F092A+868650]\n",
      "\t(No symbol) [0x00007FF6C1492FFF]\n",
      "\t(No symbol) [0x00007FF6C148F4A4]\n",
      "\t(No symbol) [0x00007FF6C148F646]\n",
      "\t(No symbol) [0x00007FF6C147EAA9]\n",
      "\tBaseThreadInitThunk [0x00007FFD6BE2E8D7+23]\n",
      "\tRtlUserThreadStart [0x00007FFD6CA1BF6C+44]\n",
      "\n",
      "ERROR FOR https://www.jstage.jst.go.jp/article/indhealth/51/1/51_2012-0089/_article/-char/ja/\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n"
     ]
    }
   ],
   "source": [
    "#function to call on all links\n",
    "os.makedirs('pdfs', exist_ok=True)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    pdf_file_path = f'pdfs/{index}.pdf'\n",
    "    if file_exists(pdf_file_path) is True:\n",
    "        print(\"PDF already downloaded\")\n",
    "    else:\n",
    "        if row['link'] in list_error_links:\n",
    "            print('error already encountered')\n",
    "        else:\n",
    "            url_returned = pdf_download(row['link'], index)  #(links[index], index)\n",
    "            if url_returned is not None:\n",
    "                list_error_links.append(url_returned)\n",
    "                print(f'ERROR FOR {url_returned}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Obtaining dependency information for pypdf from https://files.pythonhosted.org/packages/0b/27/d83f8f2a03ca5408dc2cc84b49c0bf3fbf059398a6a2ea7c10acfe28859f/pypdf-5.4.0-py3-none-any.whl.metadata\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/302.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/302.3 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/302.3 kB 640.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 286.7/302.3 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 302.3/302.3 kB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 0.pdf...\n",
      "0.pdf has 16 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 10.pdf...\n",
      "Skipping 10.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 11.pdf...\n",
      "Skipping 11.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 14.pdf...\n",
      "Skipping 14.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 15.pdf...\n",
      "Skipping 15.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 16.pdf...\n",
      "Skipping 16.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 17.pdf...\n",
      "Skipping 17.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 18.pdf...\n",
      "18.pdf has 16 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 2.pdf...\n",
      "Skipping 2.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 20.pdf...\n",
      "Skipping 20.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 22.pdf...\n",
      "Skipping 22.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 23.pdf...\n",
      "Skipping 23.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 25.pdf...\n",
      "Skipping 25.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 28.pdf...\n",
      "Skipping 28.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 3.pdf...\n",
      "Skipping 3.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 31.pdf...\n",
      "Skipping 31.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 32.pdf...\n",
      "Skipping 32.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 34.pdf...\n",
      "Skipping 34.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 35.pdf...\n",
      "Skipping 35.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 36.pdf...\n",
      "Skipping 36.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 37.pdf...\n",
      "Skipping 37.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 38.pdf...\n",
      "Skipping 38.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 39.pdf...\n",
      "Skipping 39.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 4.pdf...\n",
      "Skipping 4.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 41.pdf...\n",
      "Skipping 41.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 43.pdf...\n",
      "Skipping 43.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 46.pdf...\n",
      "Skipping 46.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 47.pdf...\n",
      "Skipping 47.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 5.pdf...\n",
      "Skipping 5.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 50.pdf...\n",
      "Skipping 50.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 51.pdf...\n",
      "Skipping 51.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 52.pdf...\n",
      "Skipping 52.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 53.pdf...\n",
      "Skipping 53.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 56.pdf...\n",
      "Skipping 56.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 6.pdf...\n",
      "6.pdf has 23 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 60.pdf...\n",
      "Skipping 60.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 61.pdf...\n",
      "Skipping 61.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 62.pdf...\n",
      "Skipping 62.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 63.pdf...\n",
      "Skipping 63.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 64.pdf...\n",
      "64.pdf has 14 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 66.pdf...\n",
      "Skipping 66.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 67.pdf...\n",
      "67.pdf has 192 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 69.pdf...\n",
      "Skipping 69.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 7.pdf...\n",
      "7.pdf has 11 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'62.45'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 72.pdf...\n",
      "Skipping 72.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 73.pdf...\n",
      "Skipping 73.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 78.pdf...\n",
      "Skipping 78.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 79.pdf...\n",
      "Skipping 79.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 8.pdf...\n",
      "Skipping 8.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 82.pdf...\n",
      "Skipping 82.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 83.pdf...\n",
      "83.pdf has 17 pages!\n",
      "Analyzing 84.pdf...\n",
      "84.pdf has 13 pages!\n",
      "Analyzing 85.pdf...\n",
      "85.pdf has 11 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 86.pdf...\n",
      "Skipping 86.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 87.pdf...\n",
      "Skipping 87.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 88.pdf...\n",
      "Skipping 88.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 9.pdf...\n",
      "Skipping 9.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 90.pdf...\n",
      "Skipping 90.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 91.pdf...\n",
      "Skipping 91.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 92.pdf...\n",
      "Skipping 92.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 93.pdf...\n",
      "93.pdf has 17 pages!\n",
      "Analyzing 94.pdf...\n",
      "94.pdf has 14 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 96.pdf...\n",
      "Skipping 96.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 97.pdf...\n",
      "97.pdf has 68 pages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 98.pdf...\n",
      "Skipping 98.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n",
      "Analyzing 99.pdf...\n",
      "Skipping 99.pdf — couldn't read PDF. Error: Stream has ended unexpectedly\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = 'pdfs'  # or your full path\n",
    "pdf_folder_content = [f for f in listdir(mypath) if isfile(join(mypath, f)) and f.lower().endswith('.pdf')]\n",
    "\n",
    "all_pdf_texts = {}\n",
    "\n",
    "for pdf in pdf_folder_content:\n",
    "    print(f'Analyzing {pdf}...')\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(join(mypath, pdf))\n",
    "        n_pages = len(reader.pages)\n",
    "        print(f'{pdf} has {n_pages} pages!')\n",
    "\n",
    "        all_text = []\n",
    "        for i in range(n_pages):  \n",
    "            page_text = reader.pages[i].extract_text()\n",
    "            all_text.append(page_text)\n",
    "\n",
    "        result_string = ' '.join(filter(None, all_text))  # remove None entries\n",
    "        all_pdf_texts[pdf] = result_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {pdf} — couldn't read PDF. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TOPIC MODELLING\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "additional_stopwords = ['et', 'al','b', 'fw', 'e'] \n",
    "\n",
    "def preprocess_text(text):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    # removes special characters and punctuation (STEP 1: Text cleaning)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text) \n",
    "    # remove additional white spaces (STEP 1: Text cleaning)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "\n",
    "    #put text in lowercases (STEP 2: Case normalization)\n",
    "    text = text.lower()\n",
    "\n",
    "    ###STEP 3: tokenization - FIX THIS \n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #STEP 4: removing stopwords:\n",
    "    stopwords_list = set(stopwords.words('english')).union(additional_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    #STEP 5: lemmeatization: \n",
    "    lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Admin/nltk_data'\n    - 'c:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_pdf_texts\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m corpus \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(processed_text)\n\u001b[0;32m      4\u001b[0m doc_term_matrix \u001b[38;5;241m=\u001b[39m [corpus\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m processed_text]\n",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m all_pdf_texts]\n\u001b[0;32m      3\u001b[0m corpus \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(processed_text)\n\u001b[0;32m      4\u001b[0m doc_term_matrix \u001b[38;5;241m=\u001b[39m [corpus\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m processed_text]\n",
      "Cell \u001b[1;32mIn[12], line 29\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     26\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#STEP 3: tokenization\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#STEP 4: removing stopwords:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m stopwords_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39munion(additional_stopwords)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Admin/nltk_data'\n    - 'c:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "processed_text = [preprocess_text(text) for text in all_pdf_texts]\n",
    "\n",
    "corpus = corpora.Dictionary(processed_text)\n",
    "doc_term_matrix = [corpus.doc2bow(text) for text in processed_text]\n",
    "\n",
    "num_topics = 45\n",
    "\n",
    "lda_model = LdaModel(doc_term_matrix, id2word = corpus, num_topics = num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "document_topics = [lda_model.get_document_topics(doc) for doc in doc_term_matrix]\n",
    "all_topics = [topic[0] for topics in document_topics for topic in topics]\n",
    "\n",
    "topics_nb = Counter(all_topics)\n",
    "\n",
    "most_nb_topics = [topic for topic, count in topics_nb.most_common()]\n",
    "print(\"The most common topics and the number of instances are:\")\n",
    "for topic, x in topics_nb.most_common():\n",
    "    print(f\"Topic {topic +1}: Count {x}\")\n",
    "for topic in most_nb_topics:\n",
    "    topics_words = lda_model.show_topic(topic)\n",
    "    most_words = [word for word, _ in topics_words]\n",
    "    print(f\"Most used words in Topic {topic +1}: {', '.join(most_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Function to compute coherence score\n",
    "def compute_coherence(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    # YOUR CODE HERE\n",
    "    coherence_values = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        lda_model = LdaModel(corpus = corpus, id2word = dictionary, num_topics = num_topics, random_state = 100)\n",
    "\n",
    "        coherence_model_lda = CoherenceModel(model = lda_model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n",
    "        coherence_values.append(coherence_model_lda.get_coherence())\n",
    "    \n",
    "    return coherence_values\n",
    "\n",
    "coherence_values = compute_coherence(dictionary=corpus, corpus= doc_term_matrix, texts=processed_text, limit=50, start = 5, step=5)\n",
    "\n",
    "optimal_nb_topics = (coherence_values.index(max(coherence_values)) + 1) * 5\n",
    "print(f\"the optimal number of topic for this model is: {optimal_nb_topics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
